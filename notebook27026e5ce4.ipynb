{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10638818,"sourceType":"datasetVersion","datasetId":6587128},{"sourceId":10638821,"sourceType":"datasetVersion","datasetId":6587131}],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-02T00:40:50.472799Z","iopub.status.idle":"2025-02-02T00:40:50.473385Z","shell.execute_reply":"2025-02-02T00:40:50.473120Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nimport time\nfrom transformers import AutoTokenizer, AutoModel\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Load datasets\npostings = pd.read_csv(\"/kaggle/input/updated-postings-csv/updated_postings.csv\")  # Adjust path for Colab\ndf_resume = pd.read_csv(\"/kaggle/input/updated-resumes-csv/updated_resumes.csv\")\n\n# Ensure 'skills_desc' column is present\nassert 'skills_desc' in postings.columns and 'skills_desc' in df_resume.columns, \"Missing 'skills_desc' column in datasets\"\n\n# Initialize TRBERT\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\nmodel = AutoModel.from_pretrained(\"bert-base-uncased\")\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n# Function to generate BERT embeddings in batches\ndef get_bert_embeddings(texts, batch_size=16):\n    embeddings = []\n    for i in range(0, len(texts), batch_size):\n        batch = texts[i:i+batch_size]\n        inputs = tokenizer(batch.tolist(), return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n        inputs = {key: val.to(device) for key, val in inputs.items()}\n        with torch.no_grad():\n            outputs = model(**inputs)\n        batch_embeddings = outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n        embeddings.extend(batch_embeddings)\n    return np.array(embeddings)\n\n# Generate embeddings using BERT\nstart_time = time.time()\npostings[\"bert_embeddings\"] = list(get_bert_embeddings(postings[\"skills_desc\"]))\ndf_resume[\"bert_embeddings\"] = list(get_bert_embeddings(df_resume[\"skills_desc\"]))\nprint(f\"BERT Embeddings Generated in {time.time() - start_time:.2f} sec.\")\n\n# LSM (Latent Semantic Matching) using TF-IDF + LSA (SVD)\nvectorizer = TfidfVectorizer(stop_words='english', max_features=5000, min_df=1, max_df=0.95)\nlsa = TruncatedSVD(n_components=100)  # Set an initial value\n\n# Fit LSM model on job postings and transform\npostings_tfidf = vectorizer.fit_transform(postings[\"skills_desc\"])\nlsa_n_components = min(postings_tfidf.shape[1], 100)  # Adjust dynamically\nlsa = TruncatedSVD(n_components=lsa_n_components)\npostings_lsm = lsa.fit_transform(postings_tfidf)\n\n# Transform resumes using the same model\nresumes_tfidf = vectorizer.transform(df_resume[\"skills_desc\"])\nresumes_lsm = lsa.transform(resumes_tfidf)\n\n# Placeholder for TallREC Model (Assuming pre-trained embeddings)\ndef get_tallrec_embeddings(texts):\n    return np.random.rand(len(texts), 100)  # Replace with actual TallREC model\n\npostings[\"tallrec_embeddings\"] = list(get_tallrec_embeddings(postings[\"skills_desc\"]))\ndf_resume[\"tallrec_embeddings\"] = list(get_tallrec_embeddings(df_resume[\"skills_desc\"]))\n\n# Fusion: Concatenation of all three embeddings\ndef fuse_embeddings(bert_emb, lsm_emb, tallrec_emb):\n    return np.hstack([bert_emb, lsm_emb, tallrec_emb])\n\npostings[\"fused_embeddings\"] = list(map(fuse_embeddings, postings[\"bert_embeddings\"], postings_lsm, postings[\"tallrec_embeddings\"]))\ndf_resume[\"fused_embeddings\"] = list(map(fuse_embeddings, df_resume[\"bert_embeddings\"], resumes_lsm, df_resume[\"tallrec_embeddings\"]))\n\n# Compute Similarities and Rankings\ndef compute_similarity(job_embeddings, resume_embeddings):\n    return cosine_similarity(resume_embeddings, job_embeddings)\n\nsimilarity_matrix = compute_similarity(np.vstack(postings[\"fused_embeddings\"]), np.vstack(df_resume[\"fused_embeddings\"]))\n\n# Ranking Function\ndef get_rankings(similarity_matrix):\n    rankings = np.argsort(-similarity_matrix, axis=1)  # Sort in descending order\n    return rankings\n\nrankings = get_rankings(similarity_matrix)\n\n# Evaluation Metrics\ndef evaluate(rankings, k_values=[1, 5, 10]):\n    num_resumes = rankings.shape[0]\n\n    # Define metrics\n    recall_at_k = {k: 0 for k in k_values}\n    ndcg_at_k = {k: 0 for k in k_values}\n    mrr = 0\n\n    for i in range(num_resumes):\n        rank_list = rankings[i]\n        first_relevant_rank = np.where(rank_list == i)[0]  # Assume ground truth is diagonal (i-th resume to i-th job)\n\n        if len(first_relevant_rank) > 0:\n            first_relevant_rank = first_relevant_rank[0] + 1  # Convert to 1-based index\n            mrr += 1 / first_relevant_rank\n\n            for k in k_values:\n                if first_relevant_rank <= k:\n                    recall_at_k[k] += 1\n                    ndcg_at_k[k] += 1 / np.log2(first_relevant_rank + 1)\n\n    # Normalize by number of resumes\n    mrr /= num_resumes\n    recall_at_k = {k: v / num_resumes for k, v in recall_at_k.items()}\n    ndcg_at_k = {k: v / num_resumes for k, v in ndcg_at_k.items()}\n\n    return recall_at_k, ndcg_at_k, mrr\n\n# Compute Metrics\nrecall, ndcg, mrr = evaluate(rankings, k_values=[1, 5, 10])\n\n# Print Evaluation Results\nprint(f\"Recall@1: {recall[1]:.4f}, Recall@5: {recall[5]:.4f}, Recall@10: {recall[10]:.4f}\")\nprint(f\"NDCG@5: {ndcg[5]:.4f}, NDCG@10: {ndcg[10]:.4f}\")\nprint(f\"MRR: {mrr:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-02T00:42:34.747209Z","iopub.execute_input":"2025-02-02T00:42:34.747565Z","iopub.status.idle":"2025-02-02T01:30:46.719709Z","shell.execute_reply.started":"2025-02-02T00:42:34.747529Z","shell.execute_reply":"2025-02-02T01:30:46.717841Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ad1c0af760d94b0094e821cef95c75da"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0ccf3c09a18248ab8f8a2881640d7676"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"05f91071d7ca42b8841f0bc0a373eefb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"499538ee79844bc384c40380c72e3f11"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e310dc808bd2451c8435db2f7bf02516"}},"metadata":{}},{"name":"stdout","text":"BERT Embeddings Generated in 2687.97 sec.\nRecall@1: 0.0000, Recall@5: 0.0000, Recall@10: 0.0000\nNDCG@5: 0.0000, NDCG@10: 0.0000\nMRR: 0.0001\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nimport time\nfrom sentence_transformers import SentenceTransformer\nfrom transformers import AutoTokenizer, AutoModel\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Load datasets (Modify paths accordingly)\npostings = pd.read_csv(\"/kaggle/input/updated-postings-csv/updated_postings.csv\")\ndf_resume = pd.read_csv(\"/kaggle/input/updated-resumes-csv/updated_resumes.csv\")\n\n# Ensure required columns exist\nassert 'skills_desc' in postings.columns and 'skills_desc' in df_resume.columns, \"Missing 'skills_desc' column in datasets\"\n\n# Initialize SBERT for better embeddings\nsbert_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Function to generate SBERT embeddings\ndef get_sbert_embeddings(texts):\n    return sbert_model.encode(texts.tolist(), convert_to_numpy=True, device=device)\n\n# Generate SBERT embeddings\nstart_time = time.time()\npostings[\"sbert_embeddings\"] = list(get_sbert_embeddings(postings[\"skills_desc\"]))\ndf_resume[\"sbert_embeddings\"] = list(get_sbert_embeddings(df_resume[\"skills_desc\"]))\nprint(f\"SBERT Embeddings Generated in {time.time() - start_time:.2f} sec.\")\n\n# TF-IDF + Latent Semantic Analysis (LSA)\nvectorizer = TfidfVectorizer(stop_words='english', max_features=5000, min_df=2, max_df=0.95)\nlsa = TruncatedSVD(n_components=100)\n\n# Fit TF-IDF and LSA on job postings\npostings_tfidf = vectorizer.fit_transform(postings[\"skills_desc\"])\nlsa_n_components = min(postings_tfidf.shape[1], 100)  # Dynamic component selection\nlsa = TruncatedSVD(n_components=lsa_n_components)\npostings_lsa = lsa.fit_transform(postings_tfidf)\n\n# Transform resumes using the same model\nresumes_tfidf = vectorizer.transform(df_resume[\"skills_desc\"])\nresumes_lsa = lsa.transform(resumes_tfidf)\n\n# Placeholder for TallREC Model (Replace with actual embeddings if available)\ndef get_tallrec_embeddings(texts):\n    return np.random.rand(len(texts), 100)  # Replace with real TallREC embeddings\n\npostings[\"tallrec_embeddings\"] = list(get_tallrec_embeddings(postings[\"skills_desc\"]))\ndf_resume[\"tallrec_embeddings\"] = list(get_tallrec_embeddings(df_resume[\"skills_desc\"]))\n\n# Fusion of SBERT, LSA, and TallREC\ndef fuse_embeddings(sbert_emb, lsa_emb, tallrec_emb):\n    return np.hstack([sbert_emb, lsa_emb, tallrec_emb])\n\npostings[\"fused_embeddings\"] = list(map(fuse_embeddings, postings[\"sbert_embeddings\"], postings_lsa, postings[\"tallrec_embeddings\"]))\ndf_resume[\"fused_embeddings\"] = list(map(fuse_embeddings, df_resume[\"sbert_embeddings\"], resumes_lsa, df_resume[\"tallrec_embeddings\"]))\n\n# Compute Similarities and Rankings\ndef compute_similarity(job_embeddings, resume_embeddings):\n    return cosine_similarity(resume_embeddings, job_embeddings)\n\nsimilarity_matrix = compute_similarity(np.vstack(postings[\"fused_embeddings\"]), np.vstack(df_resume[\"fused_embeddings\"]))\n\n# Ranking Function\ndef get_rankings(similarity_matrix):\n    return np.argsort(-similarity_matrix, axis=1)  # Descending order\n\nrankings = get_rankings(similarity_matrix)\n\n# Evaluation Metrics\ndef evaluate(rankings, k_values=[1, 5, 10]):\n    num_resumes = rankings.shape[0]\n\n    recall_at_k = {k: 0 for k in k_values}\n    ndcg_at_k = {k: 0 for k in k_values}\n    mrr = 0\n\n    for i in range(num_resumes):\n        rank_list = rankings[i]\n        first_relevant_rank = np.where(rank_list == i)[0]\n\n        if len(first_relevant_rank) > 0:\n            first_relevant_rank = first_relevant_rank[0] + 1  # Convert to 1-based index\n            mrr += 1 / first_relevant_rank\n\n            for k in k_values:\n                if first_relevant_rank <= k:\n                    recall_at_k[k] += 1\n                    ndcg_at_k[k] += 1 / np.log2(first_relevant_rank + 1)\n\n    mrr /= num_resumes\n    recall_at_k = {k: v / num_resumes for k, v in recall_at_k.items()}\n    ndcg_at_k = {k: v / num_resumes for k, v in ndcg_at_k.items()}\n\n    return recall_at_k, ndcg_at_k, mrr\n\n# Compute Metrics\nrecall, ndcg, mrr = evaluate(rankings, k_values=[1, 5, 10])\n\n# Print Evaluation Results\nprint(f\"Recall@1: {recall[1]:.4f}, Recall@5: {recall[5]:.4f}, Recall@10: {recall[10]:.4f}\")\nprint(f\"NDCG@5: {ndcg[5]:.4f}, NDCG@10: {ndcg[10]:.4f}\")\nprint(f\"MRR: {mrr:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-02T02:04:10.958847Z","iopub.execute_input":"2025-02-02T02:04:10.959221Z","iopub.status.idle":"2025-02-02T02:13:48.706125Z","shell.execute_reply.started":"2025-02-02T02:04:10.959188Z","shell.execute_reply":"2025-02-02T02:13:48.704665Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/3871 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d5707a83f8440a69977b8d6d3f41a17"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/299 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1257328d757f42108a8e7a75728846d1"}},"metadata":{}},{"name":"stdout","text":"SBERT Embeddings Generated in 383.14 sec.\nRecall@1: 0.0000, Recall@5: 0.0000, Recall@10: 0.0000\nNDCG@5: 0.0000, NDCG@10: 0.0000\nMRR: 0.0001\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"print(postings.columns)   # For job postings dataset\nprint(df_resume.columns)  # For resumes dataset\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-02T02:15:21.595374Z","iopub.execute_input":"2025-02-02T02:15:21.595906Z","iopub.status.idle":"2025-02-02T02:15:21.604837Z","shell.execute_reply.started":"2025-02-02T02:15:21.595868Z","shell.execute_reply":"2025-02-02T02:15:21.603391Z"}},"outputs":[{"name":"stdout","text":"Index(['job_id', 'company_name', 'title', 'description', 'max_salary',\n       'pay_period', 'location', 'company_id', 'views', 'med_salary',\n       'min_salary', 'formatted_work_type', 'applies', 'original_listed_time',\n       'remote_allowed', 'job_posting_url', 'application_url',\n       'application_type', 'expiry', 'closed_time',\n       'formatted_experience_level', 'skills_desc', 'listed_time',\n       'posting_domain', 'sponsored', 'work_type', 'currency',\n       'compensation_type', 'normalized_salary', 'zip_code', 'fips',\n       'sbert_embeddings', 'tallrec_embeddings', 'fused_embeddings'],\n      dtype='object')\nIndex(['address', 'career_objective', 'skills', 'educational_institution_name',\n       'degree_names', 'passing_years', 'educational_results', 'result_types',\n       'major_field_of_studies', 'professional_company_names', 'company_urls',\n       'start_dates', 'end_dates', 'related_skils_in_job', 'positions',\n       'locations', 'responsibilities', 'extra_curricular_activity_types',\n       'extra_curricular_organization_names',\n       'extra_curricular_organization_links', 'role_positions', 'languages',\n       'proficiency_levels', 'certification_providers', 'certification_skills',\n       'online_links', 'issue_dates', 'expiry_dates', 'ï»¿job_position_name',\n       'educationaL_requirements', 'experiencere_requirement',\n       'age_requirement', 'responsibilities.1', 'skills_required',\n       'matched_score', 'skills_desc', 'sbert_embeddings',\n       'tallrec_embeddings', 'fused_embeddings'],\n      dtype='object')\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Convert to string and then split into lists\npostings[\"skills_desc\"] = postings[\"skills_desc\"].astype(str).str.lower().str.split(\",\")\ndf_resume[\"skills_desc\"] = df_resume[\"skills_desc\"].astype(str).str.lower().str.split(\",\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-02T02:53:07.453366Z","iopub.execute_input":"2025-02-02T02:53:07.453829Z","iopub.status.idle":"2025-02-02T02:53:11.756163Z","shell.execute_reply.started":"2025-02-02T02:53:07.453782Z","shell.execute_reply":"2025-02-02T02:53:11.754966Z"}},"outputs":[],"execution_count":14}]}